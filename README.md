<div align="center">



</div>

<div align="center">

**è·¨æ–‡æ¡£ç§‘å­¦æ¨ç†å¼•æ“** | **ä¸æ˜¯æ£€ç´¢ç³»ç»Ÿï¼Œæ˜¯ç§‘å­¦çŸ¥è¯†æ¼”åŒ–æ¨¡å‹**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![OpenAI](https://img.shields.io/badge/LLM-OpenAI-orange.svg)](https://openai.com/)

**ä»å¤§é‡ç§‘ç ”æ–‡æ¡£ä¸­æ˜¾å¼å»ºæ¨¡ã€Œä¸»å¼ â€”å‡è®¾â€”è¯æ®ã€ï¼Œè¿›è¡Œè·¨æ–‡æ¡£å› æœé“¾ä¸çŸ¥è¯†æ¼”åŒ–æ¨ç†**

[å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢ [æ¶æ„è®¾è®¡](#-ç³»ç»Ÿæ¶æ„) â€¢ [åŠŸèƒ½ç‰¹æ€§](#-æ ¸å¿ƒåŠŸèƒ½) â€¢ [APIæ–‡æ¡£](#-apiæ¥å£) â€¢ [ç¤ºä¾‹ä»£ç ](#-ä»£ç ç¤ºä¾‹)

</div>

---

## ğŸ“– é¡¹ç›®ç®€ä»‹

### ğŸ¯ æˆ‘ä»¬ä¸æ˜¯åœ¨åš...

âŒ **å¤šç¯‡è®ºæ–‡æ‹¼æ¥ summary** - ä¸æ˜¯ç®€å•çš„æ–‡æœ¬æ‹¼æ¥  
âŒ **å•æ–‡æ¡£ QA** - ä¸æ˜¯é—®ç­”ç³»ç»Ÿ  
âŒ **å‘é‡æ£€ç´¢ + LLM ç¼–ç­”æ¡ˆ** - ä¸æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ  
âŒ **éšå¼ reasoning** - ä¸æ˜¯åœ¨ prompt é‡Œ"æƒ³ä¸€æƒ³"å°±å®Œäº‹  

### âœ… æˆ‘ä»¬åœ¨æ„å»º...

**SciDocReasoner** æ˜¯ä¸€ä¸ª**æ˜¾å¼çš„ç§‘å­¦çŸ¥è¯†æ¼”åŒ–æ¨ç†ç³»ç»Ÿ**ï¼Œå®ƒèƒ½å¤Ÿï¼š

1. **ç»“æ„åŒ–å»ºæ¨¡** ğŸ“Š
   - ä»è®ºæ–‡ä¸­æå–**ä¸»å¼ ï¼ˆClaimï¼‰**ã€**å‡è®¾ï¼ˆHypothesisï¼‰**ã€**è¯æ®ï¼ˆEvidenceï¼‰**
   - æ„å»º**å¤šæ–‡æ¡£è¯­ä¹‰æ¨ç†å›¾**ï¼Œè€Œéç®€å•çš„æ–‡æœ¬ç´¢å¼•

2. **è·¨æ–‡æ¡£æ¨ç†** ğŸ”—
   - è¯†åˆ«ä¸åŒè®ºæ–‡ä¸­çš„**å…±äº«å®ä½“**
   - å‘ç°**å†²çªä¸»å¼ **å’Œ**çŸ¥è¯†æ¼”åŒ–å…³ç³»**
   - æ¨æ–­**æœªè¢«æ˜ç¡®é™ˆè¿°çš„å‡è®¾**

3. **å›¾ä¸Šçš„æ¨ç†** ğŸ•¸ï¸
   - æ¨ç†åœ¨**å›¾ç»“æ„**ä¸Šè¿›è¡Œï¼Œè€Œéä»…åœ¨ LLM çš„ prompt ä¸­
   - LLM åªè´Ÿè´£ï¼šæŠ½å–ã€åˆ¤æ–­å…³ç³»ã€æ¨æ–­æ–°å‡è®¾
   - Graph æ˜¯ä¸€ç­‰å…¬æ°‘ï¼Œæ‰€æœ‰æ¨ç†å¯è¿½æº¯ã€å¯è°ƒè¯•

### ğŸŒŸ æ ¸å¿ƒä»·å€¼

> **SciDocReasoner is not a retrieval system.**  
> **It is a scientific knowledge evolution model**  
> **built on explicit semantic reasoning structures.**

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### ğŸ“ æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Documents Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚   PDF    â”‚  â”‚   HTML   â”‚  â”‚    MD    â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Document Parser Layer                            â”‚
â”‚  â€¢ PDF Parser  â€¢ HTML Parser  â€¢ Markdown Parser             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Preprocessing Layer                                 â”‚
â”‚  â€¢ Sentence Splitter  â€¢ Clause Extractor                    â”‚
â”‚  (ç§‘å­¦è¯­ä¹‰æœ€å°å•å…ƒåˆ‡åˆ†)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Extraction Layer (LLM-powered)                    â”‚
â”‚  â€¢ Entity Extractor  â€¢ Claim Extractor  â€¢ Hypothesis Detectorâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Linking Layer                                      â”‚
â”‚  â€¢ Cross-Document Entity Linker                             â”‚
â”‚  (å­—ç¬¦ä¸²åŒ¹é… + Embedding ç›¸ä¼¼åº¦)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Semantic Reasoning Graph                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚Document  â”‚â”€â”€â”€â”€â”€â–¶â”‚ Entity   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Claim   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚       â”‚                 â”‚                 â”‚                  â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                    â”‚Hypothesisâ”‚                             â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Hypothesis Inference Engine                           â”‚
â”‚  (ä»ç›¸å…³ claims æ¨æ–­æ–°å‡è®¾)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Query Engine                                     â”‚
â”‚  â€¢ Hypothesis Support  â€¢ Entity Evolution                     â”‚
â”‚  â€¢ Unvalidated Hypotheses  â€¢ Claim Relationships            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§© æ¨¡å—è¯¦è§£

#### 1ï¸âƒ£ æ–‡æ¡£è§£æå±‚ (`ingest/`)

**åŠŸèƒ½**ï¼šå°†ä¸åŒæ ¼å¼çš„æ–‡æ¡£è§£æä¸ºç»Ÿä¸€çš„ç»“æ„åŒ–æ ¼å¼

```python
from scidoc_reasoner.ingest import PDFParser, HTMLParser, MDParser

# PDF è§£æ
pdf_parser = PDFParser()
doc = pdf_parser.parse("paper.pdf")
# è¾“å‡º: ParsedDocument(doc_id, title, authors, sections, ...)

# HTML è§£æï¼ˆæ”¯æŒ arXivã€åšå®¢ç­‰ï¼‰
html_parser = HTMLParser()
doc = html_parser.parse("arxiv_paper.html")

# Markdown è§£æ
md_parser = MDParser()
doc = md_parser.parse("survey.md")
```

**æ·±åº¦è§£æ**ï¼š
- è‡ªåŠ¨è¯†åˆ«æ–‡æ¡£ç« èŠ‚ï¼ˆAbstract, Introduction, Methods, Results, Discussionï¼‰
- æå–å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€åˆ›å»ºæ—¥æœŸï¼‰
- ç”Ÿæˆå”¯ä¸€æ–‡æ¡£ IDï¼ˆåŸºäºæ–‡ä»¶å“ˆå¸Œï¼‰
- è¾“å‡ºç»Ÿä¸€æ ¼å¼ï¼Œä¾¿äºåç»­å¤„ç†

#### 2ï¸âƒ£ é¢„å¤„ç†å±‚ (`preprocess/`)

**åŠŸèƒ½**ï¼šå°†æ–‡æ¡£åˆ‡åˆ†ä¸ºç§‘å­¦è¯­ä¹‰æœ€å°å•å…ƒ

```python
from scidoc_reasoner.preprocess import SentenceSplitter, ClauseExtractor

# é«˜çº§å¥å­åˆ‡åˆ†ï¼ˆå¤„ç†ç§‘å­¦æ–‡æ¡£ç‰¹æ®Šæ ¼å¼ï¼‰
splitter = SentenceSplitter()
sections = [
    {"section": "Results", "raw_text": "...", "sentences": [...]}
]
sentences = splitter.split_document(doc_id, sections)

# è¯­ä¹‰å•å…ƒæå–
extractor = ClauseExtractor()
clauses = extractor.extract_clauses(sentences)
# è¯†åˆ«: assertion, comparison, causal, other
```

**æ·±åº¦è§£æ**ï¼š
- **å¥å­åˆ‡åˆ†**ï¼šå¤„ç†ç§‘å­¦ç¬¦å·ã€å¼•ç”¨ã€ç¼©å†™ï¼ˆå¦‚ "e.g.", "Fig. 1", "BERT"ï¼‰
- **è¯­ä¹‰å•å…ƒ**ï¼šè¯†åˆ«æ–­è¨€ã€æ¯”è¾ƒã€å› æœå…³ç³»çš„å¥å­
- æ¯ä¸ªå¥å­åŒ…å«å…ƒæ•°æ®ï¼š`sentence_id`, `section`, `doc_id`, `position`

#### 3ï¸âƒ£ æŠ½å–å±‚ (`extraction/`) ğŸ§ 

**åŠŸèƒ½**ï¼šä½¿ç”¨ LLM æå–å®ä½“ã€ä¸»å¼ ã€å‡è®¾

```python
from scidoc_reasoner.extraction import (
    EntityExtractor, ClaimExtractor, HypothesisDetector
)

# å®ä½“æŠ½å–
entity_extractor = EntityExtractor()
entities = entity_extractor.extract_entities(sentences)
# å®ä½“ç±»å‹: model, method, dataset, metric, biological, chemical, other

# ä¸»å¼ æŠ½å–
claim_extractor = ClaimExtractor()
claims = claim_extractor.extract_claims(sentences, entities)
# ä¸»å¼ ç±»å‹: comparative, causal, conclusive, other
# ç¤ºä¾‹: "Method A outperforms Method B on dataset X"

# å‡è®¾æ£€æµ‹
hypothesis_detector = HypothesisDetector()
hypotheses = hypothesis_detector.detect_hypotheses(sentences, claims)
# æ£€æµ‹æ˜¾å¼å‡è®¾ï¼ˆå¦‚ "we hypothesize that..."ï¼‰
```

**æ·±åº¦è§£æ**ï¼š
- **æ‰¹é‡å¤„ç†**ï¼šå°†å¥å­åˆ†æ‰¹å¤„ç†ï¼Œé¿å… token é™åˆ¶
- **ç»“æ„åŒ–è¾“å‡º**ï¼šLLM è¿”å› JSON æ ¼å¼ï¼ŒåŒ…å«ç½®ä¿¡åº¦åˆ†æ•°
- **å®ä½“å…³è”**ï¼šä¸»å¼ è‡ªåŠ¨å…³è”åˆ°æåŠçš„å®ä½“
- **ç½®ä¿¡åº¦è¯„åˆ†**ï¼šæ¯ä¸ªæŠ½å–ç»“æœéƒ½æœ‰ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰

**LLM Prompt ç¤ºä¾‹**ï¼ˆå®ä½“æŠ½å–ï¼‰ï¼š
```
Extract scientific entities from the following sentences. 
For each entity, identify:
1. The entity name/text
2. Entity type: one of ["model", "method", "dataset", "metric", ...]
3. The sentence number where it appears

Return a JSON array of entities.
```

#### 4ï¸âƒ£ é“¾æ¥å±‚ (`linking/`) ğŸ”—

**åŠŸèƒ½**ï¼šè·¨æ–‡æ¡£å®ä½“é“¾æ¥ï¼Œè¯†åˆ«ä¸åŒè®ºæ–‡ä¸­çš„åŒä¸€å®ä½“

```python
from scidoc_reasoner.linking import EntityLinker

linker = EntityLinker(model_name="all-MiniLM-L6-v2")
entity_links = linker.link_entities(all_entities)
# è¿”å›: {"Transformer": ["ent_1", "ent_5", "ent_12"], ...}
```

**æ·±åº¦è§£æ**ï¼š
- **å­—ç¬¦ä¸²åŒ¹é…**ï¼šç²¾ç¡®åŒ¹é…ã€æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†ç¼©å†™ã€åˆ«åï¼‰
- **Embedding ç›¸ä¼¼åº¦**ï¼šä½¿ç”¨ sentence-transformers è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
- **ç±»å‹çº¦æŸ**ï¼šåªé“¾æ¥ç›¸åŒç±»å‹çš„å®ä½“ï¼ˆmodel åªé“¾æ¥ modelï¼‰
- **é˜ˆå€¼æ§åˆ¶**ï¼šå¯è°ƒèŠ‚ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ 0.75ï¼‰

**é“¾æ¥ç­–ç•¥**ï¼š
1. ç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…
2. å­ä¸²åŒ¹é…ï¼ˆ"BERT" vs "Bidirectional Encoder Representations from Transformers"ï¼‰
3. è¯é‡å åº¦ > 60%
4. Embedding ä½™å¼¦ç›¸ä¼¼åº¦ > é˜ˆå€¼

#### 5ï¸âƒ£ å›¾æ„å»ºå±‚ (`graph/`) ğŸ•¸ï¸

**åŠŸèƒ½**ï¼šæ„å»ºå¤šæ–‡æ¡£è¯­ä¹‰æ¨ç†å›¾

```python
from scidoc_reasoner.graph import GraphBuilder, NodeType, EdgeType

builder = GraphBuilder()
graph = builder.build_from_documents(
    documents=all_documents,
    entities=all_entities,
    claims=all_claims,
    hypotheses=all_hypotheses,
    entity_links=entity_links
)

# å›¾ç»“æ„
# èŠ‚ç‚¹ç±»å‹: Document, Entity, Claim, Hypothesis
# è¾¹ç±»å‹: supports, contradicts, extends, based_on, mentions, contains, links_to
```

**æ·±åº¦è§£æ**ï¼š

**èŠ‚ç‚¹ç±»å‹**ï¼š
- `Document`: æ–‡æ¡£èŠ‚ç‚¹ï¼ŒåŒ…å«æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦
- `Entity`: å®ä½“èŠ‚ç‚¹ï¼ŒåŒ…å«æ–‡æœ¬ã€ç±»å‹ã€ä¸Šä¸‹æ–‡
- `Claim`: ä¸»å¼ èŠ‚ç‚¹ï¼ŒåŒ…å«æ–‡æœ¬ã€ç±»å‹ã€ç½®ä¿¡åº¦
- `Hypothesis`: å‡è®¾èŠ‚ç‚¹ï¼ŒåŒ…å«æ–‡æœ¬ã€ç½®ä¿¡åº¦ã€æ¥æºï¼ˆexplicit/inferredï¼‰

**è¾¹ç±»å‹**ï¼š
- `supports`: Claim â†’ Hypothesisï¼ˆä¸»å¼ æ”¯æŒå‡è®¾ï¼‰
- `contradicts`: Claim â†’ Claim/Hypothesisï¼ˆä¸»å¼ åé©³ï¼‰
- `extends`: Claim â†’ Claimï¼ˆä¸»å¼ å»¶ä¼¸ï¼‰
- `based_on`: Entity/Claim â†’ Hypothesis/Claimï¼ˆåŸºäºè¯æ®ï¼‰
- `mentions`: Claim/Document â†’ Entityï¼ˆæåŠå®ä½“ï¼‰
- `contains`: Document â†’ Claim/Entity/Hypothesisï¼ˆæ–‡æ¡£åŒ…å«ï¼‰
- `links_to`: Entity â†’ Entityï¼ˆè·¨æ–‡æ¡£å®ä½“é“¾æ¥ï¼‰

**å›¾æ„å»ºé€»è¾‘**ï¼š
```python
# 1. æ·»åŠ æ–‡æ¡£èŠ‚ç‚¹
for doc in documents:
    graph.add_node(f"doc_{doc_id}", node_type=NodeType.DOCUMENT, ...)

# 2. æ·»åŠ å®ä½“èŠ‚ç‚¹å¹¶é“¾æ¥
for entity in entities:
    graph.add_node(f"ent_{entity_id}", node_type=NodeType.ENTITY, ...)
    # è·¨æ–‡æ¡£é“¾æ¥
    if entity_linked:
        graph.add_edge(ent1, ent2, edge_type=EdgeType.LINKS_TO)

# 3. æ·»åŠ ä¸»å¼ èŠ‚ç‚¹å¹¶å…³è”å®ä½“
for claim in claims:
    graph.add_node(f"claim_{claim_id}", node_type=NodeType.CLAIM, ...)
    # å…³è”å®ä½“
    for entity_id in claim.entities:
        graph.add_edge(claim_node, entity_node, edge_type=EdgeType.MENTIONS)

# 4. æ·»åŠ å‡è®¾èŠ‚ç‚¹å¹¶å…³è”ä¸»å¼ 
for hypothesis in hypotheses:
    graph.add_node(f"hyp_{hyp_id}", node_type=NodeType.HYPOTHESIS, ...)
    # å…³è”æ”¯æŒçš„ä¸»å¼ 
    for claim_id in hypothesis.supporting_claims:
        graph.add_edge(claim_node, hyp_node, edge_type=EdgeType.SUPPORTS)
```

#### 6ï¸âƒ£ æ¨ç†å±‚ (`reasoning/`) ğŸ§ª

**åŠŸèƒ½**ï¼šä»ç›¸å…³ claims æ¨æ–­æ–°å‡è®¾

```python
from scidoc_reasoner.reasoning import HypothesisInferencer

inferencer = HypothesisInferencer()
inferred_hypotheses = inferencer.infer_hypotheses(
    graph, 
    min_supporting_claims=2,
    max_hypotheses=10
)

# æ¨æ–­é€»è¾‘ï¼š
# 1. æ‰¾åˆ°å…±äº«å®ä½“çš„ claim é›†ç¾¤
# 2. ä½¿ç”¨ LLM æ¨æ–­è¿™äº› claims èƒŒåçš„å…±åŒå‡è®¾
# 3. æ·»åŠ æ¨æ–­çš„å‡è®¾åˆ°å›¾ä¸­
```

**æ·±åº¦è§£æ**ï¼š

**æ¨æ–­æµç¨‹**ï¼š
1. **Claim èšç±»**ï¼š
   - é€šè¿‡å…±äº«å®ä½“èšç±» claims
   - é€šè¿‡ `EXTENDS` è¾¹æ‰¾åˆ°ç›¸å…³ claims
   - æœ€å°æ”¯æŒæ•°ï¼šè‡³å°‘ 2 ä¸ªç›¸å…³ claims

2. **LLM æ¨æ–­**ï¼š
   ```
   Given the following related scientific claims from different papers,
   infer the underlying shared hypothesis that these claims collectively support or test.
   
   Claims:
   Claim 1: "Method A outperforms Method B on dataset X"
   Claim 2: "Method A achieves 95% accuracy on long sequences"
   Claim 3: "Method B struggles with sequences longer than 512 tokens"
   
   A hypothesis should be:
   - A testable prediction or assumption
   - More general than the individual claims
   - Something that could explain or unify these claims
   ```

3. **ç»“æœéªŒè¯**ï¼š
   - ç½®ä¿¡åº¦è¯„åˆ†
   - æ¨ç†è¿‡ç¨‹è¯´æ˜
   - å…³è”åˆ°æ”¯æŒ claims

**æ¨æ–­ç¤ºä¾‹**ï¼š
```python
# è¾“å…¥ï¼šç›¸å…³ claims
claims = [
    "Sparse attention reduces memory usage by 50%",
    "Sparse attention maintains 98% accuracy",
    "Sparse attention scales to 10K tokens"
]

# æ¨æ–­çš„å‡è®¾
hypothesis = {
    "text": "Sparse attention mechanisms can efficiently scale to long sequences while maintaining model accuracy",
    "confidence": 0.82,
    "supporting_claims": ["claim_1", "claim_2", "claim_3"],
    "source": "inferred"
}
```

#### 7ï¸âƒ£ æŸ¥è¯¢å±‚ (`query/`) ğŸ”

**åŠŸèƒ½**ï¼šåœ¨æ¨ç†å›¾ä¸Šæ‰§è¡Œå¤æ‚æŸ¥è¯¢

```python
from scidoc_reasoner.query import QueryEngine

engine = QueryEngine(graph)

# æŸ¥è¯¢ 1: å‡è®¾çš„æ”¯æŒ/åé©³æƒ…å†µ
result = engine.query_hypothesis_support(
    hypothesis_text="Sparse attention improves efficiency"
)
# è¿”å›: supporting claims, contradicting claims, documents

# æŸ¥è¯¢ 2: å®ä½“çš„ç ”ç©¶æ¼”åŒ–è·¯å¾„
result = engine.query_entity_evolution(entity_name="Transformer")
# è¿”å›: evolution_path, related_claims, related_hypotheses

# æŸ¥è¯¢ 3: æœªå……åˆ†éªŒè¯çš„å‡è®¾
result = engine.query_unvalidated_hypotheses(
    min_support=2,
    max_contradictions=1
)
# è¿”å›: æ”¯æŒæ•° < 2 æˆ–åé©³æ•° > 1 çš„å‡è®¾

# æŸ¥è¯¢ 4: Claim ä¹‹é—´çš„å…³ç³»
result = engine.query_claim_relationships(claim_id="claim_123")
# è¿”å›: æ”¯æŒã€åé©³ã€å»¶ä¼¸è¯¥ claim çš„å…¶ä»– claims
```

**æ·±åº¦è§£æ**ï¼š

**æŸ¥è¯¢ 1: Hypothesis Support**
```python
def query_hypothesis_support(self, hypothesis_id, hypothesis_text):
    # 1. æ‰¾åˆ°å‡è®¾èŠ‚ç‚¹
    hyp_node = self._find_hypothesis_node(hypothesis_id, hypothesis_text)
    
    # 2. éå†å‰é©±èŠ‚ç‚¹ï¼ˆæ”¯æŒçš„ä¸»å¼ ï¼‰
    for predecessor in graph.predecessors(hyp_node):
        if edge_type == EdgeType.SUPPORTS:
            supporting_claims.append(predecessor)
    
    # 3. éå†åç»§èŠ‚ç‚¹ï¼ˆåé©³çš„ä¸»å¼ ï¼‰
    for successor in graph.successors(hyp_node):
        if edge_type == EdgeType.CONTRADICTS:
            contradicting_claims.append(successor)
    
    # 4. èšåˆæ–‡æ¡£ä¿¡æ¯
    return {
        "hypothesis": {...},
        "supporting": [...],
        "contradicting": [...],
        "documents": [...]
    }
```

**æŸ¥è¯¢ 2: Entity Evolution**
- æ‰¾åˆ°æ‰€æœ‰æåŠè¯¥å®ä½“çš„ claims
- æŒ‰æ–‡æ¡£/æ—¶é—´æ’åºï¼Œå½¢æˆæ¼”åŒ–è·¯å¾„
- å…³è”ç›¸å…³çš„å‡è®¾

**æŸ¥è¯¢ 3: Unvalidated Hypotheses**
- ç»Ÿè®¡æ¯ä¸ªå‡è®¾çš„æ”¯æŒæ•°ï¼ˆ`SUPPORTS` è¾¹ï¼‰
- ç»Ÿè®¡æ¯ä¸ªå‡è®¾çš„åé©³æ•°ï¼ˆ`CONTRADICTS` è¾¹ï¼‰
- ç­›é€‰å‡ºæ”¯æŒä¸è¶³æˆ–åé©³è¿‡å¤šçš„å‡è®¾

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ“¦ å®‰è£…

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/yourusername/scidoc-reasoner.git
cd scidoc-reasoner

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. ä¸‹è½½ spaCy æ¨¡å‹
python -m spacy download en_core_web_sm
```

### âš™ï¸ é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
OPENAI_API_KEY=your_openai_api_key_here
```

### ğŸ’» åŸºæœ¬ä½¿ç”¨

#### æ–¹å¼ 1: Python API

```python
from scidoc_reasoner.ingest import PDFParser
from scidoc_reasoner.preprocess import SentenceSplitter
from scidoc_reasoner.extraction import (
    EntityExtractor, ClaimExtractor, HypothesisDetector
)
from scidoc_reasoner.linking import EntityLinker
from scidoc_reasoner.graph import GraphBuilder
from scidoc_reasoner.reasoning import HypothesisInferencer
from scidoc_reasoner.query import QueryEngine
from scidoc_reasoner.utils.storage import StructuredStorage

# 1. è§£ææ–‡æ¡£
parser = PDFParser()
doc = parser.parse("paper1.pdf")
storage = StructuredStorage()
storage.save_document(doc.doc_id, doc.model_dump())

# 2. é¢„å¤„ç†
splitter = SentenceSplitter()
sections = [{"section": s.section, "raw_text": s.raw_text, "sentences": s.sentences} 
            for s in doc.sections]
sentences = splitter.split_document(doc.doc_id, sections)

# 3. æŠ½å–
entity_extractor = EntityExtractor()
entities = entity_extractor.extract_entities([s.model_dump() for s in sentences])

claim_extractor = ClaimExtractor()
claims = claim_extractor.extract_claims(
    [s.model_dump() for s in sentences],
    [e.model_dump() for e in entities]
)

hypothesis_detector = HypothesisDetector()
hypotheses = hypothesis_detector.detect_hypotheses(
    [s.model_dump() for s in sentences],
    [c.model_dump() for c in claims]
)

# 4. é“¾æ¥å®ä½“
linker = EntityLinker()
entity_links = linker.link_entities([e.model_dump() for e in entities])

# 5. æ„å»ºå›¾
builder = GraphBuilder()
graph = builder.build_from_documents(
    documents=[doc.model_dump()],
    entities=[e.model_dump() for e in entities],
    claims=[c.model_dump() for c in claims],
    hypotheses=[h.model_dump() for h in hypotheses],
    entity_links=entity_links
)

# 6. æ¨æ–­å‡è®¾
inferencer = HypothesisInferencer()
inferred = inferencer.infer_hypotheses(graph)
graph = inferencer.add_inferred_hypotheses_to_graph(graph, inferred)

# 7. æŸ¥è¯¢
engine = QueryEngine(graph)
results = engine.query_hypothesis_support(
    hypothesis_text="Your hypothesis here"
)
print(results)
```

#### æ–¹å¼ 2: REST API

```bash
# 1. å¯åŠ¨æœåŠ¡
uvicorn scidoc_reasoner.api.app:app --reload

# 2. ä¸Šä¼ æ–‡æ¡£
curl -X POST "http://localhost:8000/upload/pdf" \
  -F "file=@paper1.pdf"

# 3. å¤„ç†æ–‡æ¡£
curl -X POST "http://localhost:8000/process/{doc_id}"

# 4. æ„å»ºå›¾ï¼ˆå¤šæ–‡æ¡£ï¼‰
curl -X POST "http://localhost:8000/build_graph" \
  -H "Content-Type: application/json" \
  -d '{"doc_ids": ["doc_abc123", "doc_def456"]}'

# 5. æŸ¥è¯¢
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query_type": "hypothesis_support",
    "parameters": {
      "hypothesis_text": "Sparse attention improves efficiency"
    }
  }'
```

---

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. ğŸ“„ å¤šæ ¼å¼æ–‡æ¡£è§£æ

- âœ… **PDF**: æ”¯æŒå­¦æœ¯è®ºæ–‡ PDFï¼ˆæå–æ–‡æœ¬ã€å…ƒæ•°æ®ã€ç« èŠ‚ï¼‰
- âœ… **HTML**: æ”¯æŒ arXivã€åšå®¢ã€ç½‘é¡µï¼ˆBeautifulSoup è§£æï¼‰
- âœ… **Markdown**: æ”¯æŒ Markdown æ–‡æ¡£ï¼ˆæ”¯æŒ YAML frontmatterï¼‰

**ç‰¹æ€§**ï¼š
- è‡ªåŠ¨ç« èŠ‚è¯†åˆ«
- å…ƒæ•°æ®æå–ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ—¥æœŸï¼‰
- å”¯ä¸€æ–‡æ¡£ ID ç”Ÿæˆ

### 2. ğŸ”¬ ç§‘å­¦è¯­ä¹‰å•å…ƒæå–

- **å¥å­åˆ‡åˆ†**ï¼šå¤„ç†ç§‘å­¦ç¬¦å·ã€å¼•ç”¨ã€ç¼©å†™
- **è¯­ä¹‰åˆ†ç±»**ï¼šè¯†åˆ«æ–­è¨€ã€æ¯”è¾ƒã€å› æœå…³ç³»
- **ä¸Šä¸‹æ–‡ä¿ç•™**ï¼šæ¯ä¸ªå•å…ƒåŒ…å«ç« èŠ‚ã€ä½ç½®ä¿¡æ¯

### 3. ğŸ§  LLM é©±åŠ¨çš„ä¿¡æ¯æŠ½å–

- **å®ä½“æŠ½å–**ï¼šæ¨¡å‹ã€æ–¹æ³•ã€æ•°æ®é›†ã€æŒ‡æ ‡ã€ç”Ÿç‰©/åŒ–å­¦å®ä½“
- **ä¸»å¼ æŠ½å–**ï¼šæ¯”è¾ƒæ€§ã€å› æœæ€§ã€ç»“è®ºæ€§ä¸»å¼ 
- **å‡è®¾æ£€æµ‹**ï¼šæ˜¾å¼å‡è®¾è¯†åˆ«ï¼ˆ"we hypothesize that..."ï¼‰

**ä¼˜åŠ¿**ï¼š
- æ‰¹é‡å¤„ç†ä¼˜åŒ–
- ç½®ä¿¡åº¦è¯„åˆ†
- ç»“æ„åŒ– JSON è¾“å‡º

### 4. ğŸ”— è·¨æ–‡æ¡£å®ä½“é“¾æ¥

- **å­—ç¬¦ä¸²åŒ¹é…**ï¼šç²¾ç¡®åŒ¹é…ã€æ¨¡ç³ŠåŒ¹é…ã€ç¼©å†™è¯†åˆ«
- **è¯­ä¹‰ç›¸ä¼¼åº¦**ï¼šåŸºäº sentence-transformers çš„ embedding
- **ç±»å‹çº¦æŸ**ï¼šåªé“¾æ¥ç›¸åŒç±»å‹çš„å®ä½“

### 5. ğŸ•¸ï¸ å¤šæ–‡æ¡£è¯­ä¹‰æ¨ç†å›¾

**èŠ‚ç‚¹ç±»å‹**ï¼š
- `Document`: æ–‡æ¡£
- `Entity`: å®ä½“
- `Claim`: ä¸»å¼ 
- `Hypothesis`: å‡è®¾

**è¾¹ç±»å‹**ï¼š
- `supports`: æ”¯æŒå…³ç³»
- `contradicts`: åé©³å…³ç³»
- `extends`: å»¶ä¼¸å…³ç³»
- `based_on`: åŸºäºå…³ç³»
- `mentions`: æåŠå…³ç³»
- `contains`: åŒ…å«å…³ç³»
- `links_to`: é“¾æ¥å…³ç³»

### 6. ğŸ§ª å‡è®¾æ¨æ–­å¼•æ“

- **Claim èšç±»**ï¼šé€šè¿‡å…±äº«å®ä½“æˆ–å…³ç³»è¾¹èšç±»
- **LLM æ¨æ–­**ï¼šä»ç›¸å…³ claims æ¨æ–­å…±åŒå‡è®¾
- **ç½®ä¿¡åº¦è¯„ä¼°**ï¼šä¸ºæ¨æ–­çš„å‡è®¾è¯„åˆ†

### 7. ğŸ” å¼ºå¤§çš„æŸ¥è¯¢èƒ½åŠ›

**4 ç§æŸ¥è¯¢ç±»å‹**ï¼š
1. **Hypothesis Support**: æŸ¥è¯¢å‡è®¾çš„æ”¯æŒ/åé©³æƒ…å†µ
2. **Entity Evolution**: æŸ¥è¯¢å®ä½“çš„ç ”ç©¶æ¼”åŒ–è·¯å¾„
3. **Unvalidated Hypotheses**: å‘ç°æœªå……åˆ†éªŒè¯çš„å‡è®¾
4. **Claim Relationships**: æŸ¥è¯¢ claim ä¹‹é—´çš„å…³ç³»ç½‘ç»œ

---

## ğŸ“Š æ•°æ®æµç¤ºä¾‹

### å®Œæ•´æµç¨‹

```python
# è¾“å…¥ï¼š3 ç¯‡ç›¸å…³è®ºæ–‡
papers = ["paper1.pdf", "paper2.pdf", "paper3.pdf"]

# æ­¥éª¤ 1: è§£æ
docs = [PDFParser().parse(p) for p in papers]

# æ­¥éª¤ 2: é¢„å¤„ç†
all_sentences = []
for doc in docs:
    sentences = SentenceSplitter().split_document(doc.doc_id, doc.sections)
    all_sentences.extend(sentences)

# æ­¥éª¤ 3: æŠ½å–
all_entities = EntityExtractor().extract_entities(all_sentences)
all_claims = ClaimExtractor().extract_claims(all_sentences, all_entities)
all_hypotheses = HypothesisDetector().detect_hypotheses(all_sentences, all_claims)

# æ­¥éª¤ 4: é“¾æ¥
entity_links = EntityLinker().link_entities(all_entities)
# ç»“æœ: {"Transformer": ["ent_1", "ent_5"], "BERT": ["ent_2", "ent_8"]}

# æ­¥éª¤ 5: æ„å»ºå›¾
graph = GraphBuilder().build_from_documents(
    documents=docs,
    entities=all_entities,
    claims=all_claims,
    hypotheses=all_hypotheses,
    entity_links=entity_links
)
# å›¾ç»“æ„: 50 ä¸ªèŠ‚ç‚¹, 120 æ¡è¾¹

# æ­¥éª¤ 6: æ¨æ–­
inferred = HypothesisInferencer().infer_hypotheses(graph)
# æ¨æ–­å‡º 3 ä¸ªæ–°å‡è®¾

# æ­¥éª¤ 7: æŸ¥è¯¢
engine = QueryEngine(graph)
results = engine.query_entity_evolution(entity_name="Transformer")
# è¿”å›: 10 ä¸ªç›¸å…³ claims, 2 ä¸ªç›¸å…³å‡è®¾, æ¼”åŒ–è·¯å¾„
```

---

## ğŸ› ï¸ API æ¥å£

### REST API ç«¯ç‚¹

#### 1. æ–‡æ¡£ä¸Šä¼ 

```http
POST /upload/pdf
POST /upload/html
POST /upload/markdown
```

**è¯·æ±‚**ï¼š
```bash
curl -X POST "http://localhost:8000/upload/pdf" \
  -F "file=@paper.pdf"
```

**å“åº”**ï¼š
```json
{
  "doc_id": "doc_abc123",
  "title": "Attention Is All You Need",
  "status": "parsed"
}
```

#### 2. æ–‡æ¡£å¤„ç†

```http
POST /process/{doc_id}
```

**å“åº”**ï¼š
```json
{
  "doc_id": "doc_abc123",
  "sentences": 450,
  "entities": 25,
  "claims": 18,
  "hypotheses": 3,
  "status": "processed"
}
```

#### 3. æ„å»ºå›¾

```http
POST /build_graph
```

**è¯·æ±‚**ï¼š
```json
{
  "doc_ids": ["doc_abc123", "doc_def456", "doc_ghi789"]
}
```

**å“åº”**ï¼š
```json
{
  "status": "built",
  "num_nodes": 150,
  "num_edges": 320,
  "inferred_hypotheses": 5
}
```

#### 4. æŸ¥è¯¢

```http
POST /query
```

**è¯·æ±‚**ï¼š
```json
{
  "query_type": "hypothesis_support",
  "parameters": {
    "hypothesis_text": "Sparse attention improves efficiency"
  }
}
```

**å“åº”**ï¼š
```json
{
  "hypothesis": {
    "hypothesis_id": "hyp_123",
    "text": "Sparse attention improves efficiency",
    "confidence": 0.85
  },
  "supporting": [
    {
      "claim_id": "claim_1",
      "text": "Sparse attention reduces memory by 50%",
      "doc_id": "doc_abc123",
      "confidence": 0.9
    }
  ],
  "contradicting": [],
  "documents": [
    {
      "doc_id": "doc_abc123",
      "title": "Paper 1",
      "supports": true,
      "contradicts": false
    }
  ]
}
```

#### 5. å›¾ç»Ÿè®¡

```http
GET /graph/stats
```

**å“åº”**ï¼š
```json
{
  "num_nodes": 150,
  "num_edges": 320,
  "node_types": {
    "document": 3,
    "entity": 45,
    "claim": 72,
    "hypothesis": 30
  },
  "edge_types": {
    "supports": 50,
    "mentions": 120,
    "links_to": 30,
    "extends": 20,
    "contradicts": 5,
    "contains": 95
  }
}
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
scidoc_reasoner/
â”œâ”€â”€ README.md                 # é¡¹ç›®æ–‡æ¡£ï¼ˆæœ¬æ–‡ä»¶ï¼‰
â”œâ”€â”€ requirements.txt          # Python ä¾èµ–
â”œâ”€â”€ example_usage.py          # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ PROJECT_STRUCTURE.md      # é¡¹ç›®ç»“æ„è¯´æ˜
â”‚
â”œâ”€â”€ scidoc_reasoner/         # ä¸»åŒ…
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ingest/              # æ–‡æ¡£è§£æå±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py    # PDF è§£æå™¨
â”‚   â”‚   â”œâ”€â”€ html_parser.py   # HTML è§£æå™¨
â”‚   â”‚   â””â”€â”€ md_parser.py     # Markdown è§£æå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocess/          # é¢„å¤„ç†å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sentence_splitter.py  # å¥å­åˆ‡åˆ†
â”‚   â”‚   â””â”€â”€ clause_extractor.py   # è¯­ä¹‰å•å…ƒæå–
â”‚   â”‚
â”‚   â”œâ”€â”€ extraction/          # æŠ½å–å±‚ï¼ˆLLMï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ entity_extractor.py      # å®ä½“æŠ½å–
â”‚   â”‚   â”œâ”€â”€ claim_extractor.py       # ä¸»å¼ æŠ½å–
â”‚   â”‚   â””â”€â”€ hypothesis_detector.py   # å‡è®¾æ£€æµ‹
â”‚   â”‚
â”‚   â”œâ”€â”€ linking/             # é“¾æ¥å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ entity_linker.py        # è·¨æ–‡æ¡£å®ä½“é“¾æ¥
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/               # å›¾æ„å»ºå±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph_schema.py         # å›¾æ¨¡å¼å®šä¹‰
â”‚   â”‚   â””â”€â”€ graph_builder.py        # å›¾æ„å»ºå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ reasoning/           # æ¨ç†å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ hypothesis_inferencer.py # å‡è®¾æ¨æ–­å¼•æ“
â”‚   â”‚
â”‚   â”œâ”€â”€ query/               # æŸ¥è¯¢å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ query_engine.py         # æŸ¥è¯¢å¼•æ“
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                 # API å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ app.py                  # FastAPI åº”ç”¨
â”‚   â”‚
â”‚   â””â”€â”€ utils/               # å·¥å…·å±‚
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ storage.py              # ç»“æ„åŒ–å­˜å‚¨
â”‚
â””â”€â”€ data/                    # æ•°æ®ç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
    â”œâ”€â”€ storage/             # ç»“æ„åŒ–å­˜å‚¨
    â”‚   â”œâ”€â”€ documents/       # è§£æçš„æ–‡æ¡£
    â”‚   â”œâ”€â”€ entities/        # æŠ½å–çš„å®ä½“
    â”‚   â”œâ”€â”€ claims/          # æŠ½å–çš„ä¸»å¼ 
    â”‚   â””â”€â”€ graphs/          # æ„å»ºçš„å›¾
    â””â”€â”€ temp/                # ä¸´æ—¶æ–‡ä»¶
```

---

## ğŸ“ ä½¿ç”¨åœºæ™¯

### 1. æ–‡çŒ®ç»¼è¿°ç”Ÿæˆ ğŸ“š

**åœºæ™¯**ï¼šå¿«é€Ÿäº†è§£æŸä¸ªé¢†åŸŸçš„ç ”ç©¶ç°çŠ¶

```python
# è¾“å…¥ï¼š10 ç¯‡ç›¸å…³è®ºæ–‡
papers = ["transformer1.pdf", "transformer2.pdf", ...]

# å¤„ç†
graph = build_reasoning_graph(papers)

# æŸ¥è¯¢ï¼šTransformer çš„ç ”ç©¶æ¼”åŒ–
evolution = engine.query_entity_evolution("Transformer")
# è¿”å›ï¼šæŒ‰æ—¶é—´é¡ºåºçš„ä¸»å¼ ã€å‡è®¾æ¼”åŒ–è·¯å¾„
```

### 2. å‡è®¾éªŒè¯ ğŸ”¬

**åœºæ™¯**ï¼šéªŒè¯æŸä¸ªå‡è®¾æ˜¯å¦è¢«å……åˆ†æ”¯æŒ

```python
# æŸ¥è¯¢å‡è®¾æ”¯æŒæƒ…å†µ
support = engine.query_hypothesis_support(
    hypothesis_text="Sparse attention improves long-context modeling"
)

if len(support["supporting"]) < 3:
    print("âš ï¸ å‡è®¾æ”¯æŒä¸è¶³ï¼Œéœ€è¦æ›´å¤šè¯æ®")
elif len(support["contradicting"]) > 0:
    print("âš ï¸ å­˜åœ¨åé©³è¯æ®ï¼Œéœ€è¦è¿›ä¸€æ­¥éªŒè¯")
else:
    print("âœ… å‡è®¾å¾—åˆ°å……åˆ†æ”¯æŒ")
```

### 3. å‘ç°ç ”ç©¶ç©ºç™½ ğŸ•³ï¸

**åœºæ™¯**ï¼šå‘ç°æœªè¢«å……åˆ†éªŒè¯çš„å‡è®¾

```python
# æŸ¥è¯¢æœªå……åˆ†éªŒè¯çš„å‡è®¾
unvalidated = engine.query_unvalidated_hypotheses(
    min_support=2,
    max_contradictions=0
)

print(f"å‘ç° {len(unvalidated)} ä¸ªæœªå……åˆ†éªŒè¯çš„å‡è®¾")
for hyp in unvalidated:
    print(f"- {hyp['text']}")
    print(f"  æ”¯æŒæ•°: {hyp['supporting_count']}, åé©³æ•°: {hyp['contradicting_count']}")
```

### 4. çŸ¥è¯†æ¼”åŒ–è¿½è¸ª ğŸ“ˆ

**åœºæ™¯**ï¼šè¿½è¸ªæŸä¸ªæ¦‚å¿µåœ¨ä¸åŒè®ºæ–‡ä¸­çš„æ¼”åŒ–

```python
# æŸ¥è¯¢å®ä½“æ¼”åŒ–
evolution = engine.query_entity_evolution("BERT")

# è¾“å‡ºæ¼”åŒ–è·¯å¾„
for claim in evolution["evolution_path"]:
    print(f"[{claim['doc_id']}] {claim['text']}")
    print(f"  ç±»å‹: {claim['claim_type']}, ç½®ä¿¡åº¦: {claim['confidence']}")
```

---

## ğŸ”§ é…ç½®ä¸ä¼˜åŒ–

### ç¯å¢ƒå˜é‡

```bash
# .env æ–‡ä»¶
OPENAI_API_KEY=sk-...                    # OpenAI API Keyï¼ˆå¿…éœ€ï¼‰
OPENAI_MODEL=gpt-4o-mini                 # ä½¿ç”¨çš„æ¨¡å‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ gpt-4o-miniï¼‰
EMBEDDING_MODEL=all-MiniLM-L6-v2         # Embedding æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
ENTITY_LINKING_THRESHOLD=0.75            # å®ä½“é“¾æ¥é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰
```

### æ€§èƒ½ä¼˜åŒ–

1. **æ‰¹é‡å¤„ç†**ï¼šè‡ªåŠ¨å°†å¥å­/å®ä½“åˆ†æ‰¹å¤„ç†ï¼Œé¿å… token é™åˆ¶
2. **ç¼“å­˜**ï¼šç»“æ„åŒ–å­˜å‚¨æ”¯æŒç¼“å­˜ï¼Œé¿å…é‡å¤å¤„ç†
3. **å¹¶è¡Œå¤„ç†**ï¼šå¯ä»¥å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£ï¼ˆéœ€è¦æ‰‹åŠ¨å®ç°ï¼‰

### æ‰©å±•æ€§

- **è‡ªå®šä¹‰å®ä½“ç±»å‹**ï¼šä¿®æ”¹ `EntityExtractor` çš„ prompt
- **è‡ªå®šä¹‰è¾¹ç±»å‹**ï¼šåœ¨ `graph_schema.py` ä¸­æ·»åŠ æ–°çš„ `EdgeType`
- **è‡ªå®šä¹‰æŸ¥è¯¢**ï¼šåœ¨ `QueryEngine` ä¸­æ·»åŠ æ–°çš„æŸ¥è¯¢æ–¹æ³•

---

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œç¤ºä¾‹
python example_usage.py

# æµ‹è¯• API
curl http://localhost:8000/
```

---

## ğŸ“ å¼€å‘è®¡åˆ’

### âœ… MVP å·²å®Œæˆ

- [x] æ–‡æ¡£è§£æï¼ˆPDF, HTML, Markdownï¼‰
- [x] å¥å­åˆ‡åˆ†ä¸è¯­ä¹‰å•å…ƒæå–
- [x] å®ä½“ã€ä¸»å¼ ã€å‡è®¾æŠ½å–
- [x] è·¨æ–‡æ¡£å®ä½“é“¾æ¥
- [x] å¤šæ–‡æ¡£è¯­ä¹‰æ¨ç†å›¾æ„å»º
- [x] å‡è®¾æ¨æ–­å¼•æ“
- [x] 4 ç§æŸ¥è¯¢ç±»å‹
- [x] REST API æ¥å£

### ğŸš§ åç»­æ‰©å±•

- [ ] **æ—¶é—´æ¼”åŒ–åˆ†æ**ï¼šè¿½è¸ªå‡è®¾éšæ—¶é—´çš„å˜åŒ–
- [ ] **ç½®ä¿¡åº¦è¡°å‡/å¼ºåŒ–**ï¼šæ ¹æ®æ–°è¯æ®æ›´æ–°ç½®ä¿¡åº¦
- [ ] **å¼•ç”¨åŠ æƒæ¨ç†**ï¼šè€ƒè™‘å¼•ç”¨å…³ç³»çš„é‡è¦æ€§
- [ ] **å®¡ç¨¿äººå¼çŸ›ç›¾æ£€æµ‹**ï¼šè‡ªåŠ¨å‘ç°è®ºæ–‡é—´çš„çŸ›ç›¾
- [ ] **å¯è§†åŒ–ç•Œé¢**ï¼šå›¾ç»“æ„å¯è§†åŒ–å·¥å…·
- [ ] **æ‰¹é‡å¤„ç†ä¼˜åŒ–**ï¼šæ”¯æŒå¤§è§„æ¨¡æ–‡æ¡£é›†ï¼ˆ100+ è®ºæ–‡ï¼‰

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·å…ˆé˜…è¯» [CONTRIBUTING.md](CONTRIBUTING.md)ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ™ è‡´è°¢

- [OpenAI](https://openai.com/) - æä¾› LLM API
- [NetworkX](https://networkx.org/) - å›¾æ•°æ®ç»“æ„
- [FastAPI](https://fastapi.tiangolo.com/) - Web æ¡†æ¶
- [sentence-transformers](https://www.sbert.net/) - Embedding æ¨¡å‹

---

## ğŸ‘¤ ä½œè€… (Author)

**Haoze Zheng**

*   ğŸ“ **School**: Xinjiang University (XJU)
*   ğŸ“§ **Email**: zhenghaoze@stu.xju.edu.cn
*   ğŸ± **GitHub**: [mire403](https://github.com/mire403)

---

<div align="center">

**å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Starï¼**

<sub>Made by Haoze Zheng. 2026 VoiceDataExplorer.</sub>

</div>





